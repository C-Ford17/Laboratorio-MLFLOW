{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789ec019-957f-42e7-b786-447bf36695b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: ollama in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: openai in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.26.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.184.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.41.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (5.28.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.11.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\christian\\appdata\\roaming\\python\\python311\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core->google-generativeai) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\christian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai ollama openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f6cff0-1bae-4747-8e1d-2ca1360029e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key cargada correctamente: AIzaSy... (oculta por seguridad)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Leer la variable del entorno\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"No se encontrÃ³ GOOGLE_API_KEY en el archivo .env\")\n",
    "\n",
    "print(f\"API Key cargada correctamente: {api_key[:6]}... (oculta por seguridad)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcc2550-f72f-4274-ab05-efe8a2e0c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/15 22:08:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'llm_gemini_chat'.\n",
      "Created version '1' of model 'llm_gemini_chat'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini registrado â†’ runs:/70339e75db774ca38d7418d709c2a113/model\n",
      "ðŸ•’ Latencia: 10.61s | Tokens: 1289 | Costo: $0.001934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/15 22:08:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama registrado â†’ runs:/eb7198f3fc704b698699dde0116b1087/model\n",
      "ðŸ•’ Latencia: 2.04s | Tokens: 10 | Costo: $0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'llm_ollama_chat'.\n",
      "Created version '1' of model 'llm_ollama_chat'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import random\n",
    "import google.generativeai as genai\n",
    "import requests\n",
    "\n",
    "# Configurar Gemini\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# Crear experimento\n",
    "mlflow.set_experiment(\"laboratorio_3_llms\")\n",
    "\n",
    "# --- FunciÃ³n para simular costo (puedes ajustarla) ---\n",
    "def estimate_cost(tokens, provider):\n",
    "    if provider.lower() == \"gemini\":\n",
    "        # Costos aproximados (USD) basados en tarifas reales de texto\n",
    "        return tokens * 0.0000015\n",
    "    elif provider.lower() == \"ollama\":\n",
    "        # Asumimos costo casi nulo o simulado\n",
    "        return tokens * 0.0000001\n",
    "    return 0\n",
    "\n",
    "# --- FunciÃ³n para ejecutar y registrar el modelo ---\n",
    "def run_llm(provider, model_name, temperature, task_type, prompt):\n",
    "    with mlflow.start_run(run_name=f\"{provider}_run\") as run:\n",
    "        start = time.time()\n",
    "\n",
    "        if provider.lower() == \"gemini\":\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(prompt)\n",
    "            text_response = response.text\n",
    "            total_tokens = response.usage_metadata.total_token_count if hasattr(response, \"usage_metadata\") else len(prompt.split()) + len(text_response.split())\n",
    "        elif provider.lower() == \"ollama\":\n",
    "            # Llama al endpoint local de Ollama\n",
    "            res = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json={\"model\": model_name, \"prompt\": prompt, \"stream\": False}\n",
    "            )\n",
    "            response_json = res.json()\n",
    "            text_response = response_json.get(\"response\", \"\")\n",
    "            total_tokens = len(prompt.split()) + len(text_response.split())\n",
    "        else:\n",
    "            raise ValueError(\"Proveedor no soportado\")\n",
    "\n",
    "        latency = time.time() - start\n",
    "        cost = estimate_cost(total_tokens, provider)\n",
    "\n",
    "        # Registrar parÃ¡metros\n",
    "        mlflow.log_param(\"modelo\", model_name)\n",
    "        mlflow.log_param(\"proveedor\", provider)\n",
    "        mlflow.log_param(\"temperatura\", temperature)\n",
    "        mlflow.log_param(\"tipo_tarea\", task_type)\n",
    "\n",
    "        # Registrar mÃ©tricas\n",
    "        mlflow.log_metric(\"latencia\", latency)\n",
    "        mlflow.log_metric(\"total_tokens\", total_tokens)\n",
    "        mlflow.log_metric(\"costo_estimado\", cost)\n",
    "\n",
    "        # Guardar artefactos\n",
    "        with open(f\"prompt_{provider}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(prompt)\n",
    "        with open(f\"respuesta_{provider}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text_response)\n",
    "\n",
    "        mlflow.log_artifact(f\"prompt_{provider}.txt\")\n",
    "        mlflow.log_artifact(f\"respuesta_{provider}.txt\")\n",
    "\n",
    "        # Registrar modelo simbÃ³lico\n",
    "        class DummyLLM(mlflow.pyfunc.PythonModel):\n",
    "            def predict(self, context, model_input):\n",
    "                return [text_response]\n",
    "\n",
    "        model_name_registry = f\"llm_{provider.lower()}_chat\"\n",
    "        model_info = mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=DummyLLM(),\n",
    "            registered_model_name=model_name_registry\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… {provider} registrado â†’ {model_info.model_uri}\")\n",
    "        print(f\"ðŸ•’ Latencia: {latency:.2f}s | Tokens: {total_tokens} | Costo: ${cost:.6f}\")\n",
    "\n",
    "# --- Ejecutar pruebas reales ---\n",
    "prompt = \"ExplÃ­came brevemente quÃ© es el aprendizaje profundo en inteligencia artificial.\"\n",
    "run_llm(\"Gemini\", \"gemini-2.5-flash\", 0.7, \"chat\", prompt)\n",
    "run_llm(\"Ollama\", \"llama3.1\", 0.6, \"chat\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e354e7-a9ec-4531-bedc-23e0a5e2270d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
